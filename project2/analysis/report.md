2.1 Pourquoi est-il difficile d'estimer l'erreur r√©siduelle ?
L'erreur r√©siduelle, not√©e 
ùúé
2
œÉ 
2
 , correspond au bruit irr√©ductible dans les donn√©es, li√© √† des facteurs al√©atoires ou non mesur√©s. Dans notre contexte, plusieurs √©l√©ments rendent son estimation difficile :

Subjectivit√© des √©valuations : Le dataset Wine Quality repose sur des jugements humains pour noter la qualit√© des vins. Ces notes sont influenc√©es par les pr√©f√©rences et perceptions personnelles des d√©gustateurs, ce qui ajoute un bruit important et impr√©visible aux donn√©es.

Inconnaissance de 
ùëì
(
ùë•
)
f(x) : La fonction id√©ale 
ùëì
(
ùë•
)
f(x), qui lie les caract√©ristiques des vins √† leur qualit√©, est inconnue. Cela nous emp√™che de s√©parer pr√©cis√©ment 
ùúé
2
œÉ 
2
  des autres composantes de l‚Äôerreur.

Facteurs non mesur√©s : Des variables comme l‚Äôorigine du vin ou les conditions de d√©gustation ne sont pas incluses dans les donn√©es, ce qui contribue √† l'erreur r√©siduelle sans qu‚Äôon puisse la mod√©liser.

En pratique, 
ùúé
2
œÉ 
2
  reste constant pour un dataset donn√©. Plut√¥t que de l‚Äôestimer directement, nous nous concentrons sur la somme biais + erreur r√©siduelle, suffisante pour comparer les performances des mod√®les.


## 2. **Question 2.2 - Protocole d'estimation**

Pour estimer la variance, l'erreur attendue et la somme du biais et de l'erreur r√©siduelle √† partir du pool de donn√©es P, nous proposons un protocole en quatre √©tapes principales. Ce protocole est con√ßu pour obtenir des estimations fiables tout en tenant compte des contraintes inh√©rentes au probl√®me.

**√âtape 1 : Pr√©paration initiale des donn√©es**

Cette premi√®re √©tape est cruciale pour garantir la validit√© de nos estimations :
- Division du jeu de donn√©es complet (NS = 6497 √©chantillons) en :
  * Un pool P pour l'apprentissage (80% des donn√©es)
  * Un ensemble de test fixe (20% des donn√©es)

*Justification* :
- La s√©paration train/test est essentielle pour √©valuer la v√©ritable capacit√© de g√©n√©ralisation des mod√®les
- L'ensemble de test doit rester fixe pour assurer la coh√©rence des comparaisons entre diff√©rents mod√®les
- La proportion 80/20 est un standard qui offre un bon compromis entre la taille du pool d'apprentissage et la fiabilit√© des estimations sur l'ensemble de test

**√âtape 2 : Cr√©ation des √©chantillons d'apprentissage**

Cette √©tape vise √† simuler la variabilit√© naturelle dans l'apprentissage :
- √Ä partir du pool P, nous g√©n√©rons B = 100 √©chantillons d'apprentissage diff√©rents
- Chaque √©chantillon contient N = 250 observations
- La s√©lection est faite al√©atoirement sans remise pour chaque √©chantillon

*Justification* :
- B = 100 it√©rations offrent un bon compromis entre pr√©cision statistique et temps de calcul
- N = 250 est suffisamment grand pour l'apprentissage mais reste petit par rapport √† NS
- L'√©chantillonnage sans remise √©vite les duplications au sein d'un m√™me √©chantillon
- La r√©p√©tition du processus B fois permet de capturer la variabilit√© due √† l'√©chantillonnage

**√âtape 3 : Entra√Ænement et pr√©diction**

Pour chaque √©chantillon d'apprentissage i de 1 √† B :
1. Entra√Ænement d'un nouveau mod√®le fi sur l'√©chantillon i
2. Pr√©dictions fi(x) sur l'ensemble de test fixe
3. Stockage des pr√©dictions dans une matrice de dimension B √ó M (M √©tant la taille de l'ensemble de test)

*Justification* :
- L'utilisation d'un nouveau mod√®le √† chaque it√©ration assure l'ind√©pendance des apprentissages
- Les pr√©dictions sur l'ensemble de test fixe permettent une comparaison coh√©rente
- Le stockage des pr√©dictions permet un calcul efficace des m√©triques finales

**√âtape 4 : Calcul des estimations**

Cette √©tape finale permet d'obtenir nos trois m√©triques d'int√©r√™t :

1. **Variance** :
```python
variance = moyenne(variance_des_predictions_pour_chaque_point)
```
*Justification* : La variance mesure la dispersion des pr√©dictions pour un m√™me point, moyenn√©e sur tous les points de test. Elle quantifie l'instabilit√© du mod√®le face √† diff√©rents √©chantillons d'apprentissage.

2. **Erreur attendue** :
```python
erreur_attendue = moyenne((predictions - vraies_valeurs)¬≤)
```
*Justification* : L'erreur quadratique moyenne mesure la performance globale du mod√®le, prenant en compte √† la fois le biais et la variance.

3. **Biais¬≤ + Erreur r√©siduelle** :
```python
biais_plus_residuel = erreur_attendue - variance
```
*Justification* : Bien que nous ne puissions pas s√©parer le biais de l'erreur r√©siduelle, leur somme nous permet d'√©valuer la part de l'erreur qui n'est pas due √† la variance.

**Avantages du protocole :**

1. **Robustesse** :
   - Les estimations sont moyenn√©es sur de nombreux √©chantillons
   - L'utilisation d'un ensemble de test fixe assure la coh√©rence des comparaisons

2. **Flexibilit√©** :
   - Le protocole s'adapte √† diff√©rents types de mod√®les
   - Les param√®tres B et N peuvent √™tre ajust√©s selon les besoins

3. **Efficacit√©** :
   - Permet d'estimer les composantes cl√©s de l'erreur
   - Facilite l'analyse du compromis biais-variance

4. **Praticit√©** :
   - Simple √† impl√©menter
   - R√©sultats faciles √† interpr√©ter

Ce protocole, bien que ne permettant pas d'isoler l'erreur r√©siduelle, nous fournit toutes les informations n√©cessaires pour analyser comment les hyperparam√®tres des diff√©rentes m√©thodes affectent le compromis biais-variance, ce qui est l'objectif principal de notre √©tude.


## 3. **Question 2.3 - Impl√©mentation et analyse**

**3.1 Impl√©mentation du protocole**

Notre impl√©mentation du protocole d'estimation se compose de deux fichiers principaux :

**bias_variance.py** : Contient les fonctions fondamentales d'estimation
```python
def estimate_bias_variance(X, y, estimator, n_samples=250, n_iterations=100):
    """
    Impl√©mente le protocole d'estimation en quatre √©tapes :
    1. S√©paration train/test (80/20)
    2. G√©n√©ration de B=100 √©chantillons de taille N=250
    3. Entra√Ænement des mod√®les et pr√©dictions
    4. Calcul des m√©triques (variance, erreur attendue, biais¬≤+r√©siduel)
    """
```

```python
def evaluate_model_complexity(X, y, create_model, param_range):
    """
    √âvalue l'impact des hyperparam√®tres sur le compromis biais-variance :
    - Teste diff√©rentes valeurs d'hyperparam√®tres
    - Applique estimate_bias_variance pour chaque configuration
    - Retourne les courbes d'√©volution des m√©triques
    """
```

**main.py** : Applique le protocole aux trois mod√®les demand√©s
1. k-NN : variation du nombre de voisins k
2. Lasso : variation du param√®tre de r√©gularisation Œª
3. Arbres de d√©cision : variation de la profondeur maximale

Les hyperparam√®tres ont √©t√© choisis pour couvrir un large spectre de complexit√© :
- k-NN : k ‚àà [1, 3, 5, 7, 11, 15, 21, 31, 51]
- Lasso : Œª ‚àà [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
- Arbres : profondeur ‚àà [1, 2, 3, 5, 7, 10, 15, 20, None]

Pour chaque configuration, le code :
1. Cr√©e le mod√®le avec les hyperparam√®tres sp√©cifi√©s
2. Applique le protocole d'estimation
3. Calcule et stocke les trois m√©triques
4. G√©n√®re des visualisations du compromis biais-variance

Cette impl√©mentation respecte fid√®lement le protocole d√©fini pr√©c√©demment tout en offrant la flexibilit√© n√©cessaire pour l'analyse comparative des diff√©rentes m√©thodes d'apprentissage.


R√©sultats :

=== k-NN Analysis ===
k (n_neighbors)=1: Error=0.9584, Variance=0.4810, Bias¬≤+Residual=0.4774
k (n_neighbors)=3: Error=0.6607, Variance=0.1671, Bias¬≤+Residual=0.4936
k (n_neighbors)=5: Error=0.6050, Variance=0.1021, Bias¬≤+Residual=0.5030
k (n_neighbors)=7: Error=0.5846, Variance=0.0738, Bias¬≤+Residual=0.5108
k (n_neighbors)=11: Error=0.5710, Variance=0.0481, Bias¬≤+Residual=0.5229
k (n_neighbors)=15: Error=0.5684, Variance=0.0360, Bias¬≤+Residual=0.5324
k (n_neighbors)=21: Error=0.5705, Variance=0.0262, Bias¬≤+Residual=0.5443
k (n_neighbors)=31: Error=0.5782, Variance=0.0181, Bias¬≤+Residual=0.5601
k (n_neighbors)=51: Error=0.5931, Variance=0.0115, Bias¬≤+Residual=0.5816

=== Lasso Analysis ===
alpha=0.0001: Error=0.5706, Variance=0.0302, Bias¬≤+Residual=0.5404
alpha=0.001: Error=0.5696, Variance=0.0294, Bias¬≤+Residual=0.5403
alpha=0.01: Error=0.5642, Variance=0.0232, Bias¬≤+Residual=0.5410
alpha=0.1: Error=0.5805, Variance=0.0086, Bias¬≤+Residual=0.5719
alpha=1.0: Error=0.7379, Variance=0.0036, Bias¬≤+Residual=0.7342
alpha=10.0: Error=0.7379, Variance=0.0036, Bias¬≤+Residual=0.7342
alpha=100.0: Error=0.7379, Variance=0.0036, Bias¬≤+Residual=0.7342

=== Decision Trees Analysis ===
max_depth=1: Error=0.6561, Variance=0.0451, Bias¬≤+Residual=0.6110
max_depth=2: Error=0.6394, Variance=0.0868, Bias¬≤+Residual=0.5526
max_depth=3: Error=0.6773, Variance=0.1447, Bias¬≤+Residual=0.5326
max_depth=5: Error=0.7951, Variance=0.2896, Bias¬≤+Residual=0.5055
max_depth=7: Error=0.9083, Variance=0.4185, Bias¬≤+Residual=0.4898
max_depth=10: Error=1.0100, Variance=0.5276, Bias¬≤+Residual=0.4825
max_depth=15: Error=1.0527, Variance=0.5679, Bias¬≤+Residual=0.4848
max_depth=20: Error=1.0526, Variance=0.5695, Bias¬≤+Residual=0.4831
max_depth=None: Error=1.0526, Variance=0.5695, Bias¬≤+Residual=0.4831


**3.2 Analyse des r√©sultats**

Les r√©sultats obtenus pour les trois mod√®les montrent des comportements distincts et coh√©rents avec la th√©orie du compromis biais-variance.

**1. k-NN Analysis**

Observations cl√©s :
- Pour k=1 : Error=0.9584, Variance=0.4810, Bias¬≤+Residual=0.4774
- Pour k=15 : Error=0.5684, Variance=0.0360, Bias¬≤+Residual=0.5324
- Pour k=51 : Error=0.5931, Variance=0.0115, Bias¬≤+Residual=0.5816

Analyse :
- La variance diminue de mani√®re monotone avec l'augmentation de k (de 0.4810 √† 0.0115)
- Le biais¬≤ + r√©siduel augmente progressivement avec k (de 0.4774 √† 0.5816)
- L'erreur totale atteint son minimum autour de k=15 (0.5684)
- Ces r√©sultats sont coh√©rents avec la th√©orie :
  * k faible : mod√®le flexible ‚Üí haute variance, bas biais
  * k √©lev√© : mod√®le rigide ‚Üí basse variance, haut biais
  * Le k optimal (‚âà15) repr√©sente le meilleur compromis

**2. Lasso Analysis**

Observations cl√©s :
- Pour Œ±=0.0001 : Error=0.5706, Variance=0.0302, Bias¬≤+Residual=0.5404
- Pour Œ±=0.01 : Error=0.5642, Variance=0.0232, Bias¬≤+Residual=0.5410
- Pour Œ±=100.0 : Error=0.7379, Variance=0.0036, Bias¬≤+Residual=0.7342

Analyse :
- La variance diminue avec l'augmentation de Œ±
- Le biais augmente significativement pour Œ± > 0.1
- L'erreur minimale est atteinte pour Œ±=0.01
- Comportement notable :
  * Stabilisation des m√©triques pour Œ± ‚â• 1.0
  * Faible impact sur la variance pour Œ± tr√®s petit
  * Le compromis optimal est plus sensible que pour k-NN

**3. Decision Trees Analysis**

Observations cl√©s :
- Pour depth=1 : Error=0.6561, Variance=0.0451, Bias¬≤+Residual=0.6110
- Pour depth=3 : Error=0.6773, Variance=0.1447, Bias¬≤+Residual=0.5326
- Pour depth=None : Error=1.0526, Variance=0.5695, Bias¬≤+Residual=0.4831

Analyse :
- Augmentation dramatique de la variance avec la profondeur
- Diminution du biais avec la profondeur
- Comportement le plus extr√™me des trois mod√®les :
  * La variance passe de 0.0451 √† 0.5695
  * L'erreur totale se d√©grade significativement
  * Signe clair de surapprentissage pour les grandes profondeurs

**Comparaison globale**

1. **Stabilit√©** :
   - k-NN : le plus stable dans l'√©volution des m√©triques
   - Lasso : transition brusque autour de Œ±=0.1
   - Arbres : √©volution la plus instable

2. **Meilleure performance** :
   - k-NN : 0.5684 (k=15)
   - Lasso : 0.5642 (Œ±=0.01)
   - Arbres : 0.6394 (depth=2)

3. **Compromis biais-variance** :
   - k-NN offre le compromis le plus √©quilibr√©
   - Lasso maintient une variance faible
   - Arbres montrent la plus grande sensibilit√© √† la complexit√©

Ces r√©sultats confirment les principes th√©oriques du compromis biais-variance et montrent l'importance du choix des hyperparam√®tres pour chaque mod√®le.


## 4. **Question 2.4 - Sample size analyse**

**Mise en place du test**

Pour √©tudier l'impact de la taille d'√©chantillon sur le compromis biais-variance, nous avons adapt√© notre protocole initial avec les modifications suivantes :

1. **S√©lection des tailles d'√©chantillon** :
   - Nous testons une gamme de tailles : [50, 100, 250, 500, 1000, 2000]
   - Cette progression quasi-g√©om√©trique permet d'observer l'√©volution sur diff√©rents ordres de grandeur
   - La limite sup√©rieure (2000) reste inf√©rieure √† la taille totale du dataset (6497)

2. **Configuration des mod√®les** :
   - k-NN : k fix√© √† 15 (valeur optimale identifi√©e pr√©c√©demment)
   - Lasso : Œª fix√© √† 0.01 (valeur optimale identifi√©e pr√©c√©demment)
   - Arbres de d√©cision : deux configurations test√©es
     * Profondeur fixe (3) pour un mod√®le contraint
     * Profondeur non limit√©e (None) pour un mod√®le flexible

3. **Processus d'√©valuation** :
   - Pour chaque taille d'√©chantillon N :
     * Application du protocole d'estimation (B=100 it√©rations)
     * Calcul des trois m√©triques (erreur, variance, biais¬≤+r√©siduel)
     * Conservation des r√©sultats pour chaque mod√®le

4. **Visualisation** :
   - Graphiques en √©chelle logarithmique pour la taille d'√©chantillon
   - Courbes s√©par√©es pour chaque mod√®le
   - Affichage des trois m√©triques sur chaque graphique

Cette approche nous permet d'observer comment la taille d'√©chantillon influence le compromis biais-variance pour diff√©rents types de mod√®les, tout en maintenant les autres param√®tres constants.

Resultats :

R√©sultats pour knn:
  Taille √©chantillon | Erreur    | Variance  | Biais¬≤ + R√©siduel
  --------------------|-----------|-----------|------------------
                50 |    0.6481 |    0.0439 |    0.6042
               100 |    0.6101 |    0.0409 |    0.5693
               250 |    0.5684 |    0.0360 |    0.5324
               500 |    0.5464 |    0.0337 |    0.5128
              1000 |    0.5265 |    0.0295 |    0.4970
              2000 |    0.5090 |    0.0233 |    0.4857

R√©sultats pour lasso:
  Taille √©chantillon | Erreur    | Variance  | Biais¬≤ + R√©siduel
  --------------------|-----------|-----------|------------------
                50 |    0.6970 |    0.1567 |    0.5402
               100 |    0.6024 |    0.0603 |    0.5421
               250 |    0.5642 |    0.0232 |    0.5410
               500 |    0.5511 |    0.0101 |    0.5410
              1000 |    0.5460 |    0.0044 |    0.5417
              2000 |    0.5435 |    0.0018 |    0.5417

R√©sultats pour tree_fixed:
  Taille √©chantillon | Erreur    | Variance  | Biais¬≤ + R√©siduel
  --------------------|-----------|-----------|------------------
                50 |    0.9489 |    0.4137 |    0.5352
               100 |    0.7929 |    0.2615 |    0.5315
               250 |    0.6773 |    0.1447 |    0.5326
               500 |    0.6195 |    0.0874 |    0.5322
              1000 |    0.5941 |    0.0541 |    0.5401
              2000 |    0.5805 |    0.0317 |    0.5488

R√©sultats pour tree_full:
  Taille √©chantillon | Erreur    | Variance  | Biais¬≤ + R√©siduel
  --------------------|-----------|-----------|------------------
                50 |    1.1918 |    0.6674 |    0.5244
               100 |    1.0940 |    0.5892 |    0.5048
               250 |    1.0526 |    0.5695 |    0.4831
               500 |    1.0083 |    0.5517 |    0.4567
              1000 |    0.9667 |    0.5293 |    0.4374
              2000 |    0.8832 |    0.4794 |    0.4038

**4.2 Analyse des r√©sultats**

Les r√©sultats montrent clairement l'impact de la taille d'√©chantillon sur le compromis biais-variance pour les diff√©rents mod√®les.

**1. k-NN (k=15)**

Observations cl√©s :
- N=50 : Error=0.6481, Variance=0.0439, Bias¬≤+Residual=0.6042
- N=2000 : Error=0.5090, Variance=0.0233, Bias¬≤+Residual=0.4857

Analyse :
- Diminution r√©guli√®re de la variance (-47% de N=50 √† N=2000)
- R√©duction progressive du biais¬≤+r√©siduel (-20%)
- Am√©lioration constante de l'erreur totale
- Comportement stable et pr√©visible :
  * La variance diminue de mani√®re quasi-lin√©aire avec log(N)
  * Le biais diminue plus lentement mais r√©guli√®rement
  * Mod√®le le plus robuste aux variations de taille d'√©chantillon

**2. Lasso (Œ±=0.01)**

Observations cl√©s :
- N=50 : Error=0.6970, Variance=0.1567, Bias¬≤+Residual=0.5402
- N=2000 : Error=0.5435, Variance=0.0018, Bias¬≤+Residual=0.5417

Analyse :
- R√©duction drastique de la variance (-99% de N=50 √† N=2000)
- Biais¬≤+r√©siduel remarquablement stable (‚âà0.54)
- Convergence rapide :
  * La variance chute rapidement jusqu'√† N=500
  * Stabilisation presque compl√®te apr√®s N=1000
  * Meilleure performance finale (0.5435)

**3. Arbres de d√©cision**

a) **Profondeur fixe (depth=3)**
- N=50 : Error=0.9489, Variance=0.4137, Bias¬≤+Residual=0.5352
- N=2000 : Error=0.5805, Variance=0.0317, Bias¬≤+Residual=0.5488

Analyse :
- Forte r√©duction de la variance (-92%)
- Biais¬≤+r√©siduel stable
- Am√©lioration significative de l'erreur totale (-39%)

b) **Profondeur non limit√©e**
- N=50 : Error=1.1918, Variance=0.6674, Bias¬≤+Residual=0.5244
- N=2000 : Error=0.8832, Variance=0.4794, Bias¬≤+Residual=0.4038

Analyse :
- Variance toujours √©lev√©e m√™me avec N grand
- Diminution notable du biais¬≤+r√©siduel (-23%)
- Surapprentissage persistant :
  * La variance reste √©lev√©e m√™me avec N=2000
  * L'erreur totale reste la plus √©lev√©e des quatre configurations

**Comparaison globale**

1. **Impact sur la variance** :
   - Tous les mod√®les montrent une diminution de la variance avec N
   - Lasso : r√©duction la plus spectaculaire (-99%)
   - Arbres non limit√©s : r√©duction la plus faible (-28%)

2. **Stabilit√© du biais** :
   - Lasso : biais le plus stable
   - k-NN : l√©g√®re am√©lioration avec N
   - Arbres non limit√©s : am√©lioration significative mais insuffisante

3. **Efficacit√© de l'apprentissage** :
   - Lasso : meilleure performance finale (0.5435)
   - k-NN : bon compromis (0.5090)
   - Arbres fixes : am√©lioration notable (0.5805)
   - Arbres non limit√©s : toujours probl√©matiques (0.8832)

Ces r√©sultats confirment plusieurs principes th√©oriques :
1. La variance diminue g√©n√©ralement avec 1/N
2. Le biais est moins sensible √† la taille d'√©chantillon
3. Les mod√®les plus complexes n√©cessitent plus de donn√©es
4. La r√©gularisation (Lasso, k-NN) am√©liore la stabilit√©

Cette analyse d√©montre l'importance cruciale de la taille d'√©chantillon dans le compromis biais-variance, particuli√®rement pour les mod√®les complexes ou non r√©gularis√©s.